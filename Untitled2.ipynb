{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9135222-5494-453e-9944-580b74495df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Universe is a very strange place. It conatins planets, stars, etc. We live on the planet. It is the only planet where water is in liquid form. Liquid water is very essential for life to grow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8c6efb-dc8e-48b9-90b5-f1bab0027d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Universe is a very strange place. It conatins planets, stars, etc. We live on the planet. It is the only planet where water is in liquid form. Liquid water is very essential for life to grow.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2010b817-c0de-4793-b23d-5842597033a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universe is a very strange place.', 'It conatins planets, stars, etc.', 'We live on the planet.', 'It is the only planet where water is in liquid form.', 'Liquid water is very essential for life to grow.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e15baa74-4abc-41f6-a6b9-5f1990c30b2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pos_tag' from 'nltk.tokenize' (C:\\Users\\Sanket Rathod\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, pos_tag\n\u001b[0;32m      2\u001b[0m tokenized_word\u001b[38;5;241m=\u001b[39mword_tokenize(text)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_word)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pos_tag' from 'nltk.tokenize' (C:\\Users\\Sanket Rathod\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c31503-ff2e-4c04-852a-e8431273c9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wouldn', 'doesn', 'couldn', \"we'd\", 'mightn', 'he', \"you'd\", \"she's\", 'we', \"it'll\", \"they're\", \"i'd\", 'do', 'herself', 'more', 'because', 'its', 'so', 'at', \"shouldn't\", 'where', 'few', 'no', 'theirs', 're', 'most', 'have', 've', 'ma', 'with', 'on', 'are', \"won't\", 'being', 'when', 'in', 'into', 'the', 'they', 'shan', \"we're\", 'by', 'your', 'from', 'weren', 'below', 'whom', 'but', \"they'd\", 'through', 'having', 'about', 'between', \"haven't\", \"she'd\", 'didn', 'here', 'him', 'our', 'own', 'yours', 'was', 'her', \"isn't\", 'o', 's', 'm', \"he's\", 'above', \"wasn't\", \"that'll\", \"it'd\", 'all', 'she', \"you'll\", 'did', \"he'd\", 'i', 'there', \"he'll\", 'too', 'not', 'to', 'this', 'and', 'myself', 'out', 'wasn', \"didn't\", 'which', 'same', 'such', 'me', 'what', 'hers', 'only', 'an', 'down', 'should', 'then', 'or', \"mustn't\", 'while', 'now', 'yourself', \"we've\", 'a', 'been', 'until', 'that', 'aren', 'just', \"she'll\", 'any', 'haven', \"mightn't\", 'ain', 'be', 'who', \"doesn't\", 'how', 'am', \"hadn't\", 'himself', 'very', \"wouldn't\", 'some', 'these', 'over', 'of', 'after', 'those', 'my', \"don't\", \"i'll\", 'ours', 'hadn', 'can', \"shan't\", \"you're\", 'yourselves', 'you', 'as', 'off', 'during', 'them', 'each', \"aren't\", 'd', \"weren't\", 'their', 'why', \"i'm\", 'up', 'once', \"couldn't\", 'does', \"they'll\", 'further', 'itself', \"they've\", 'has', 'other', 'won', \"hasn't\", 'before', 'needn', 'against', \"needn't\", 'mustn', 'don', \"we'll\", 'is', 'had', 'isn', 'both', 'll', 'it', 'were', \"it's\", 'if', 'nor', 'for', 'than', 'y', 'his', \"you've\", 'shouldn', \"i've\", 'hasn', \"should've\", 't', 'ourselves', 'themselves', 'under', 'will', 'doing', 'again'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2244af0-5f3c-49ab-9c46-ff4bb83690e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text= \"How to remove stop words with NLTK library in Python?\"\n",
    "text= re.sub('[^a-zA-Z]', ' ',text)\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_text = [w for w in tokens if w not in stop_words]\n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f45c6625-aa88-4159-b5c6-c4f94253c0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words = [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps = PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord = ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a4edbb-2fbd-45b2-9fb8-a58222678383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies: study\n",
      "Lemma for studying: studying\n",
      "Lemma for cries: cry\n",
      "Lemma for cry: cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {}: {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e0a2788-e705-448e-8def-ef9eb569b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('pink', 'NN'), ('sweater', 'NN'), ('fit', 'VBP'), ('her', 'PRP$'), ('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "data = \"The pink sweater fit her perfectly\"\n",
    "words = word_tokenize(data)\n",
    "tags = pos_tag(words)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5deb397a-d5b2-4c1d-aaaa-a97407d85cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfff6f8a-88db-4eb4-b9e8-062393345973",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2299595e-d576-411d-b16c-16a1841a5975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jupiter', 'is', 'the', 'largest', 'Planet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfWordsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b04889ad-a1f3-41d0-be72-0b711fec30bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mars', 'is', 'the', 'fourth', 'planet', 'from', 'the', 'Sun']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfWordsB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a499980-50a5-4fbc-af1d-9cc043a90764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter',\n",
       " 'Mars',\n",
       " 'Planet',\n",
       " 'Sun',\n",
       " 'fourth',\n",
       " 'from',\n",
       " 'is',\n",
       " 'largest',\n",
       " 'planet',\n",
       " 'the'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a4841b1-a343-42ef-bffe-80587590802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fc1515c-a249-41ee-a8c3-b57524e66944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter': 1,\n",
       " 'Mars': 0,\n",
       " 'largest': 1,\n",
       " 'planet': 0,\n",
       " 'from': 0,\n",
       " 'is': 1,\n",
       " 'fourth': 0,\n",
       " 'Sun': 0,\n",
       " 'Planet': 1,\n",
       " 'the': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOfWordsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4511a9d-0bc9-4f53-8626-cf98e287d40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter': 0,\n",
       " 'Mars': 1,\n",
       " 'largest': 0,\n",
       " 'planet': 1,\n",
       " 'from': 1,\n",
       " 'is': 1,\n",
       " 'fourth': 1,\n",
       " 'Sun': 1,\n",
       " 'Planet': 0,\n",
       " 'the': 2}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOfWordsB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d779d59-641f-4bc5-9bc9-2e8c3a939862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e9c4807-1021-49ca-9590-3c740a8f3ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter': 0.2,\n",
       " 'Mars': 0.0,\n",
       " 'largest': 0.2,\n",
       " 'planet': 0.0,\n",
       " 'from': 0.0,\n",
       " 'is': 0.2,\n",
       " 'fourth': 0.0,\n",
       " 'Sun': 0.0,\n",
       " 'Planet': 0.2,\n",
       " 'the': 0.2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e5c0c59-b8cd-482e-9d78-231d5f68990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter': 0.0,\n",
       " 'Mars': 0.125,\n",
       " 'largest': 0.0,\n",
       " 'planet': 0.125,\n",
       " 'from': 0.125,\n",
       " 'is': 0.125,\n",
       " 'fourth': 0.125,\n",
       " 'Sun': 0.125,\n",
       " 'Planet': 0.0,\n",
       " 'the': 0.25}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4969fb81-c9f9-43f0-8ffc-85dec4306f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89233af9-9ab7-4e97-ba52-81a29ec03505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter': 0.6931471805599453,\n",
       " 'Mars': 0.6931471805599453,\n",
       " 'largest': 0.6931471805599453,\n",
       " 'planet': 0.6931471805599453,\n",
       " 'from': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'fourth': 0.6931471805599453,\n",
       " 'Sun': 0.6931471805599453,\n",
       " 'Planet': 0.6931471805599453,\n",
       " 'the': 0.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ec6a6c8-09c3-443c-a713-daab8a574702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Jupiter      Mars   largest    planet      from   is    fourth       Sun  \\\n",
      "0  0.138629  0.000000  0.138629  0.000000  0.000000  0.0  0.000000  0.000000   \n",
      "1  0.000000  0.086643  0.000000  0.086643  0.086643  0.0  0.086643  0.086643   \n",
      "\n",
      "     Planet  the  \n",
      "0  0.138629  0.0  \n",
      "1  0.000000  0.0  \n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3148f49-3898-44d1-a6c3-aa4f79432e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
